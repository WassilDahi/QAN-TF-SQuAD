{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QAN_TF_SQuAD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOd2Mkky2YzHMtqxK3OrjSP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehdi-karimi-math/QAN-TF-SQuAD/blob/master/QAN_TF_SQuAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_PwZNySm7zI",
        "colab_type": "text"
      },
      "source": [
        "# **Question-Answering Network (QAN) designed in TensorFlow 2, based on SQuAD**\n",
        "\n",
        "\n",
        "---\n",
        "In this Colab notebook, I show how to design a simple question-answering (QA) system all in TensorFlow 2 and Keras, and in the notebook format. My goal is to cover the basics of how to handle and process text data in TensorFlow, how to use embedding, and how to define the layers and design a simple QA deep-learning model. There are many other resources online, but I only use the APIs in TensorFlow and try to cover the basics. This notebook can be used as a framwork for more complicated and efficient QANs. \n",
        "\n",
        "To start learning about Natural Language Processing (NLP), I recommend the following courses in Coursera and Stanford, I learned so much from them.\n",
        "\n",
        "[Coursera's Natural Language Processing in TensorFlow](https://www.coursera.org/learn/natural-language-processing-tensorflow)\n",
        "\n",
        "[Stanford's NLP with Deep Learning](http://onlinehub.stanford.edu/cs224)\n",
        "\n",
        "If you have any comments, do not hesitate to contact me (Mehdi Karimi) at mahdikarimi1982 at gmail. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhT5UTdLOW5N",
        "colab_type": "text"
      },
      "source": [
        "The machine learning platform I am using is [TensorFlow 2.0](https://www.tensorflow.org/). To install this version, we can use the following lines of code and then check if it is correctly installed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgZVZe_8RW-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxXU2M7wRbaP",
        "colab_type": "code",
        "outputId": "81f2209f-d254-4593-eed2-c54ecd338830",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaMsHvtdSues",
        "colab_type": "text"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "---\n",
        "\n",
        "TensorFlow is working on top of a higher level API called Keras. We first inport keras and numpy. For handling data, we use tensorflow_datasets ([tfds](https://www.tensorflow.org/datasets/api_docs/python/tfds)). tfds is a collection of modules to deal with data. It also contains a collection of datasets ready to use with TensorFlow. These datasets have different difficulites for both practice and research. We are going to load [squad](https://www.tensorflow.org/datasets/catalog/squad). From the website, SQuAd is a \"reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage\". So the answer to each question is a span of the text and, as we will see, we only care about the start and end positions of the answer. The following commands load this dataset as a dictionary, where the train and validation parts are already separated. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2PAOpFiVsmc",
        "colab_type": "code",
        "outputId": "0fb8fcbb-7a3b-499e-c4b4-da283be86718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "squad_data, info = tfds.load(\"squad\", with_info=True)\n",
        "\n",
        "squad_train = squad_data['train']\n",
        "squad_valid = squad_data['validation']\n",
        "\n",
        "print(info.features)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FeaturesDict({\n",
            "    'answers': Sequence({\n",
            "        'answer_start': Tensor(shape=(), dtype=tf.int32),\n",
            "        'text': Text(shape=(), dtype=tf.string),\n",
            "    }),\n",
            "    'context': Text(shape=(), dtype=tf.string),\n",
            "    'id': Tensor(shape=(), dtype=tf.string),\n",
            "    'question': Text(shape=(), dtype=tf.string),\n",
            "    'title': Text(shape=(), dtype=tf.string),\n",
            "})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgSpFQN8Yg64",
        "colab_type": "text"
      },
      "source": [
        "We see that the training data is a dictionary with 6 items. \"answers\" value is again another dictionary with two items. The first one is an array of the start of the answers in the context (character count), and the second item is the text of the answers. In the main dictionary, we also have an item for the \"context\" and \"question\", which we are going to use.  Let us read and put the data we need in separate lists. \n",
        "\n",
        "Note that we change the tensors into numpy array to later use the text processing of Keras. Also pay attention to the slicing of the arrays. All the texts are started by characters \"b'...\", which we do not need. It will be usefull to play with data before and after the processing. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY2GrVIMZNR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context=[] # This list contains the context we are asking question about. \n",
        "question=[] # This list contains the text of the questions. \n",
        "answer_t=[] # This list containts the text of the answers. \n",
        "answer_i=[] # This list contains the start of the answers. \n",
        "\n",
        "# The same lists for the validation data. \n",
        "context_val=[]\n",
        "question_val=[]\n",
        "answer_t_val=[]\n",
        "answer_i_val=[]\n",
        "\n",
        "for ques in squad_train:\n",
        "  context.append(str(ques['context'].numpy())[2:-1])\n",
        "  question.append(str(ques['question'].numpy())[2:-1])\n",
        "  answer_t.append(str(ques['answers']['text'].numpy())[3:-2])\n",
        "  answer_i.append(ques['answers']['answer_start'].numpy())\n",
        "\n",
        "for ques in squad_valid:\n",
        "  context_val.append(str(ques['context'].numpy())[2:-1])\n",
        "  question_val.append(str(ques['question'].numpy())[2:-1])\n",
        "  answer_t_val.append(str(ques['answers']['text'].numpy())[3:-2])\n",
        "  answer_i_val.append(ques['answers']['answer_start'].numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYV1ac5se-E4",
        "colab_type": "code",
        "outputId": "ae1d34f3-dfc4-419b-ab8c-c521b69c83ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# we can check the content of the lists. \n",
        "index = 0\n",
        "print(context[index])\n",
        "print(question[index])\n",
        "print(answer_t[index], answer_i[index])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The difference in the above factors for the case of \\xce\\xb8=0 is the reason that most broadcasting (transmissions intended for the public) uses vertical polarization. For receivers near the ground, horizontally polarized transmissions suffer cancellation. For best reception the receiving antennas for these signals are likewise vertically polarized. In some applications where the receiving antenna must work in any position, as in mobile phones, the base station antennas use mixed polarization, such as linear polarization at an angle (with both vertical and horizontal components) or circular polarization.\n",
            "What is one use that would require an antenna to receive signals in various ways at once?\n",
            "mobile phones [427]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGLPcgbiibQQ",
        "colab_type": "text"
      },
      "source": [
        "# Text Preprocessing\n",
        "---\n",
        "For the processing of the text, we use Keras Preprocessing, which is the data preprocessing module of Keras. We use the [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) class that let us make a dictionary of all the words in the context list, and change text into a sequence of integers. We also use padding: for our deep learning model, we want all the context (question) vectors be of the same size, which we put equal to the maximum size and pad the other vectors by zeros. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRwqjMkpfdsI",
        "colab_type": "code",
        "outputId": "58d92b14-294e-48c0-a3ec-64286321be0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padding_type='post' # This means we pad the zeors at the end of each vector. \n",
        "oov_tok = \"<OOV>\"  # If for a vector, a word in not in the vocanbulary, this will be used. \n",
        "\n",
        "\n",
        "tokenizer = Tokenizer( oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(context)\n",
        "word_index = tokenizer.word_index  # This will be our dictionary of all the words. \n",
        "num_words = len(word_index.keys())\n",
        "print(\"number of words in our dictionary is {}\".format(num_words))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of words in our dictionary is 82780\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6gNQJeRlUHN",
        "colab_type": "code",
        "outputId": "aeb371e8-858f-4581-88a6-c259e2faca9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# change the context vectors into integer vectors and padding them. \n",
        "sequences = tokenizer.texts_to_sequences(context)\n",
        "con_len = max(map(len, sequences))\n",
        "print(\"max length of a context vector is {}\". format(con_len))\n",
        "context_padded = pad_sequences(sequences,maxlen=con_len, padding=padding_type)\n",
        "\n",
        "# change the question vectors into integer vectors and padding them. \n",
        "sequences = tokenizer.texts_to_sequences(question)\n",
        "que_len = max(map(len, sequences))\n",
        "print(\"max length of a question vector is {}\". format(que_len))\n",
        "question_padded = pad_sequences(sequences,maxlen=que_len, padding=padding_type)\n",
        "\n",
        "# change the answer vectors into integer vectors\n",
        "answer_token = tokenizer.texts_to_sequences(answer_t)\n",
        "\n",
        "### We can do th same process for the validation vectors. \n",
        "\n",
        "# sequences_val = tokenizer.texts_to_sequences(context_val)\n",
        "# context_val_padded = pad_sequences(sequences,maxlen=con_len, padding=padding_type)\n",
        "# sequences_val = tokenizer.texts_to_sequences(question_val)\n",
        "# question_val_padded = pad_sequences(sequences,maxlen=que_len, padding=padding_type)\n",
        "# answer_token_val = tokenizer.texts_to_sequences(answer_t_val)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max length of a context vector is 718\n",
            "max length of a question vector is 40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd4ckzQ6UfGL",
        "colab_type": "text"
      },
      "source": [
        "The following part is important in our processing. As we mentioned, we are interested in the start and end positions of answers. We also have the character count for the start of the answer. We can use the \"split\" method of python to count the number of words, but this does not necessarily match our tokenized vector. Hence, we search around the estimated position to find the exact one, and if we do not find it, we neglect the question. At the end, we get our \"cleaned\" data. You can perform a more detailed cleaning.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BURAZdzjqSfx",
        "colab_type": "code",
        "outputId": "3d1b84f1-6ba3-4815-cba7-cc1ae915697b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train_i = [] # This list contains tuples of length two, (start,end), for the answers. \n",
        "\n",
        "context_padded_clean=[]\n",
        "question_padded_clean=[]\n",
        "answer_t_clean=[]\n",
        "\n",
        "for i in range(len(answer_i)):\n",
        "  temp = answer_i[i][0]\n",
        "  start = len(context[i][0:temp-1].replace('-',' ').split())+1\n",
        "  temp_l = len(answer_t[i].replace('-',' ').split())\n",
        "  end = start+temp_l-1\n",
        "  flag = False\n",
        "  for j in range(start-10,start+10):\n",
        "    if np.array_equal(context_padded[i][j:j+temp_l], answer_token[i]):\n",
        "      start = j\n",
        "      end = j+temp_l-1\n",
        "      flag = True\n",
        "  if flag:\n",
        "    y_train_i.append((start,end))\n",
        "    context_padded_clean.append(context_padded[i])\n",
        "    question_padded_clean.append(question_padded[i])\n",
        "    answer_t_clean.append(answer_t[i])\n",
        "\n",
        "context_padded_clean=np.array(context_padded_clean)\n",
        "question_padded_clean=np.array(question_padded_clean)\n",
        "answer_t_clean=np.array(answer_t_clean)\n",
        "\n",
        "num_train_data = context_padded_clean.shape[0]\n",
        "print(\"number of training samples after cleaning is = {}\".format(num_train_data))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training samples after cleaning is = 74590\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yF1vZHbWnkQ",
        "colab_type": "text"
      },
      "source": [
        "Now we create the label vector for our training. Here, context vectors have size 718, this label vector has size 2*718. The first half is a one-hot vector for the start position of the answer, and the second half is for the end position. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFsr--Guw_IO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = []\n",
        "\n",
        "for i in range(len(context_padded_clean)):\n",
        "    \n",
        "    s_ = np.zeros(con_len,dtype = \"float32\")\n",
        "    e_ = np.zeros(con_len,dtype = \"float32\")\n",
        "    \n",
        "    s_[y_train_i[i][0]] = 1\n",
        "    e_[y_train_i[i][1]] = 1\n",
        "    y_train.append(np.concatenate((s_,e_)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6RCjiS6xHtO",
        "colab_type": "text"
      },
      "source": [
        "# Embedding \n",
        "---\n",
        "\n",
        "The idea of embedding is an interesting concept in NLP. You can find a short [tutorial](https://www.tensorflow.org/tutorials/text/word_embeddings) in  Tensorflow website. When we encode the words as numbers, we want to assign a vector to each word. The naive way is to assign one-hot vectors to the words. The problem is that the vectors are too large, and there is no connection between similar words. By embedding, we give a vector in $R^m$ to each word, where $m$ is a reasonable number, in a way that similar words are close to each other in this embedding. Good news is that there are already pretrained embeddings that we can load and use in our models. We use [GloVe](https://nlp.stanford.edu/projects/glove/) embedding of size 50. Using largers sizes can improve the performance, but for a longer training time. \n",
        "\n",
        "There are [different ways](https://colab.research.google.com/notebooks/io.ipynb) to load a file into a Colab notebook. An efficient way is mounting your google drive into your Colab, which we also use later for trianing. Then we can put our file (in this case you can google for 'glove.6B.50d.txt') into 'My drive' directory and read it into Colab. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjBdCQxIxLK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O61fzqIoxYqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = dict()\n",
        "\n",
        "f = open('/content/drive/My Drive/glove.6B.50d.txt')\n",
        "glove = 50 # the dimension of embedding\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OC9ZVkS1gX4",
        "colab_type": "text"
      },
      "source": [
        "Now we create an embedding matrix for the words in our dictionary. This matrix is used in our embedding layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhqh2WhExgWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = np.zeros((num_words+1, glove))\n",
        "for word, index in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss6PuUNpx7hI",
        "colab_type": "text"
      },
      "source": [
        "# Defining the layers and the model\n",
        "---\n",
        "\n",
        "At this point, we have processed our data, and are ready to design a model. First note that threre two input lines of data, one for context and one for question. As we know, for answering a question about a text, we need information about the whole squence of words. In other words, we want to connect previous information to a present task. Recurrent neural networks address this goal. We are going to use LSTM layers to learn long-term dependencies. \n",
        "\n",
        "In the following, we define two classes. One is \"Embedding\", which is basically the embedding matrix and is not trainable. The other one is \"Bi_LSTM\", which is a built-in bidirectional LSTM layer of Keras. Bidirectioal means information can go both forward and backward. A prameter we input to LSTM is UNITS, which is the dimensionality of the output space for the LSTM layer. \n",
        "\n",
        "The layer that we costum is the \"BiLinear_Layer\", which models the **Attention**. [Here](https://www.tensorflow.org/guide/keras/custom_layers_and_models) is a link on how to custom your layers in TensorFlow. Attention is an important concept in NLP for question-answering and translation. We have two input streams for context and question, and attention tells us how to connect these two. The beginning of the input for our model is as follows:\n",
        "\n",
        "$$\n",
        "\\text{Context} \\rightarrow \\text{Embedding} \\rightarrow \\text{LSTM}  \\rightarrow H_c\n",
        "$$\n",
        "$$\n",
        "\\text{Question} \\rightarrow \\text{Embedding} \\rightarrow \\text{LSTM}  \\rightarrow H_q\n",
        "$$\n",
        "\n",
        "We use a simple bilinear type attention. In our model, the length of a context vector is 718 and the length of a question vector is 40. Let the number of LSTM UNITS be $U$.  $H_c$ is a matrix of size $718 \\times U$ and $H_q$ is a matrix of size $40 \\times U$. For the start position of the answer, we define a weight matrix $W_S$ of size $U \\times U$, which gives us\n",
        "\n",
        "$$\n",
        "H := H_c W_S H_q^\\top.\n",
        "$$\n",
        "\n",
        "Now $H$ is a matrix of size $718 \\times 40$. We apply another weight vector $F_S$ to compute $H F_S$, which is a vector of length 718. We pass this vector through a softmax function to create an output vector of probabilities for the start position of the answer. $W_E$ and $F_E$ are also defined similarly for the end position of the answer. The output of this layer is the concatenation of these two probability vectors. \n",
        "\n",
        "**Important Note:** To improve the model for this task, a more complicated attention must be used. This bilinear attention we use here is a very simple one. There are many resources in the literature about attention. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91XQE5kux2F1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Bidirectional,LSTM,Dense,Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "class Embedding(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    This class defines the embedding layer. The weights are comming from the embedding matrix and are not trainable. \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_words , embedding_matrix , embedding_dim = glove):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(num_words+1, embedding_dim, weights=[embedding_matrix], trainable=False, mask_zero=True)\n",
        "\n",
        "    def call(self, x):\n",
        "        \n",
        "        x = self.embedding(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Bi_LSTM(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    This class defines bidirectional LSTM we are going to use for both the context and questions. \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, UNITS):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.bilstm = Bidirectional(LSTM(UNITS, return_sequences=True, recurrent_initializer='glorot_uniform', dropout = 0.4))\n",
        "        \n",
        "    def call(self,x):\n",
        "        \n",
        "        output = self.bilstm(x)\n",
        "        return output\n",
        "  \n",
        "\n",
        "\n",
        "class BiLinear_Layer(tf.keras.Model):\n",
        "     def __init__(self, UNITS, que_len):\n",
        "        \"\"\"\n",
        "        This class defines the bilnear layer which performs the attention. It combines the output of the LSTM layers from the context and quesion side. \n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.WS = tf.Variable(initial_value=w_init(shape=(UNITS, UNITS), dtype='float32'), trainable=True)\n",
        "        # self.FS = tf.Variable(initial_value=np.ones((que_len, 1), dtype='float32'), trainable=True)\n",
        "        self.FS = tf.Variable(initial_value=w_init(shape=(que_len, 1), dtype='float32') , trainable=True)\n",
        "        \n",
        "\n",
        "        self.WE = tf.Variable(initial_value=w_init(shape=(UNITS, UNITS), dtype='float32'), trainable=True)\n",
        "        # self.FE = tf.Variable(initial_value=np.ones((que_len, 1), dtype='float32') , trainable=True)\n",
        "        self.FE = tf.Variable(initial_value=w_init(shape=(que_len, 1), dtype='float32') , trainable=True)\n",
        "\n",
        "\n",
        "     def call(self,con_mat,que_mat):\n",
        "        \n",
        "        start_temp = con_mat @ self.WS\n",
        "        start_temp = start_temp @ tf.transpose(que_mat, [0,2,1])\n",
        "        start_temp = start_temp @ self.FS\n",
        "        start_temp = tf.nn.softmax(start_temp, axis = 1)\n",
        "\n",
        "\n",
        "        end_temp = con_mat @ self.WE\n",
        "        end_temp = end_temp @ tf.transpose(que_mat,[0,2,1])\n",
        "        end_temp = end_temp @ self.FE\n",
        "        end_temp = tf.nn.softmax(end_temp, axis = 1)\n",
        "\n",
        "        prob = tf.concat([start_temp,end_temp],axis=1)\n",
        "        return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzYn0NSaMb6d",
        "colab_type": "text"
      },
      "source": [
        "To design our model, as we discussed, both context and question vectors go through embedding and LSTM layers. The outputs of these layers go into the bilinear layer. We can see a summary of our model in the following. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EauioQzw0wBp",
        "colab_type": "code",
        "outputId": "6632b191-e298-4511-895f-7d61775fc8a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "UNITS = 128\n",
        "\n",
        "c_input = Input(shape=(con_len,))\n",
        "c_emb = Embedding(num_words, embedding_matrix)(c_input)\n",
        "c_lstm = Bi_LSTM(UNITS)(c_emb)\n",
        "\n",
        "q_input = Input(shape=(que_len,))\n",
        "q_emb = Embedding(num_words, embedding_matrix)(q_input)\n",
        "q_lstm = Bi_LSTM(UNITS)(q_emb)\n",
        "\n",
        "\n",
        "y_prob = BiLinear_Layer(2*UNITS, que_len)(c_lstm, q_lstm)\n",
        "\n",
        "model = Model(inputs = [c_input, q_input],outputs =y_prob)\n",
        "model.summary()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 718)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 40)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 718, 50)      4139050     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 40, 50)       4139050     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bi_lstm (Bi_LSTM)               (None, 718, 256)     183296      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bi_lstm_1 (Bi_LSTM)             (None, 40, 256)      183296      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bi_linear__layer (BiLinear_Laye (None, 1436, 1)      131152      bi_lstm[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 8,775,844\n",
            "Trainable params: 497,744\n",
            "Non-trainable params: 8,278,100\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfJ-lUkwoLOT",
        "colab_type": "text"
      },
      "source": [
        "# Loss function and costum accuracy\n",
        "---\n",
        "For the loss function, we use the categorical crossntropy loss function, but for both the start position and the end position. The final loss is the summation of these two. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoPW3P48oQ21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Loss(y_true, prob):\n",
        "\n",
        "    \"\"\"\n",
        "    This function calculates the loss for our model. We basically calcualte the loss for the start and end positions and add them together. \n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    # breaking the lists into two half's for the start and end positions.\n",
        "    start_label = y_true[:,:con_len]\n",
        "    end_label = y_true[:,con_len:]\n",
        "    \n",
        "    start_logit = prob[:,:con_len]\n",
        "    end_logit = prob[:,con_len:]\n",
        "    \n",
        "    start_loss = tf.keras.backend.categorical_crossentropy(start_label,start_logit)\n",
        "    end_loss = tf.keras.backend.categorical_crossentropy(end_label,end_logit)\n",
        "    \n",
        "    return start_loss + end_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-wG92hMobOP",
        "colab_type": "text"
      },
      "source": [
        "The slightly more complicated part is defining an accuracy measure. There are several built-in [metrics](https://keras.io/metrics/) in Keras (you can also see the Tensorflow ones [here](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)). But for this model, we have to define a specific one. There is template [here](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric) on how to costum a metric using Matric class. In the following \"Custom_Accuracy\" class, we check which start and end positions are predicted by the model's output. We calculate accuracy as the average of the number of correct start and end labels. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiZmypxcoeNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Custom_Accuracy(keras.metrics.Metric):\n",
        "\n",
        "    def __init__(self, nm, name='costum_accuracy',  **kwargs):\n",
        "      super().__init__(name=name, **kwargs)\n",
        "      self.nm = nm\n",
        "      self.accuracy = self.add_weight(name='tp', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred,  sample_weight=None):\n",
        "      len_ = np.shape(y_pred)[1]\n",
        "      \n",
        "      y_1 = y_pred[:,:len_ //2]\n",
        "      y_2 = y_pred[:,len_ //2:]\n",
        "      # the predicted labels are the ones with the highest probability. \n",
        "      y_1 = tf.reshape(tf.argmax(y_1, axis=1), shape=(-1, 1))\n",
        "      y_2 = tf.reshape(tf.argmax(y_2, axis=1), shape=(-1, 1))\n",
        "\n",
        "      y_1t = y_true[:,:len_ // 2]\n",
        "      y_2t = y_true[:,len_ // 2:]\n",
        "      y_1t = tf.reshape(tf.argmax(y_1t, axis=1), shape=(-1, 1))\n",
        "      y_2t = tf.reshape(tf.argmax(y_2t, axis=1), shape=(-1, 1))\n",
        "\n",
        "      values1 = tf.cast(y_1, 'int32') == tf.cast(y_1t, 'int32')\n",
        "      values1 = tf.cast(values1, 'float32')\n",
        "\n",
        "      values2 = tf.cast(y_2, 'int32') == tf.cast(y_2t, 'int32')\n",
        "      values2 = tf.cast(values2, 'float32')\n",
        "\n",
        "      self.accuracy.assign_add( (tf.reduce_sum(values1+values2)) / (tf.dtypes.cast(2*self.nm, tf.float32)))\n",
        "\n",
        "    def result(self):\n",
        "      return self.accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9x6ekSy1tS5",
        "colab_type": "text"
      },
      "source": [
        "# Training the Model\n",
        "---\n",
        "\n",
        "We use Dataset class and the function \"from_tensor_slices\" to convert our numpy lists into a more efficient Tensorflow dateset. Pay attention to how we do it for multiple inputs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCr8NlmZ1zNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "question_padded_ = np.array(question_padded_clean)\n",
        "context_padded_ = np.array(context_padded_clean)\n",
        "y_train_ = np.array(y_train)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(( {\"input_1\" : context_padded_ , \"input_2\" : question_padded_}, y_train_))\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "# SHUFFLE_BUFFER_SIZE = 100\n",
        "\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=Loss, metrics=[Custom_Accuracy(num_train_data)])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yphHEtRQ30vJ",
        "colab_type": "text"
      },
      "source": [
        "Now we are ready to train our model. Unfortunately, the training of this model takes time, and  most likely the Colab gets disconnected before the end of training, so you lose your data. One way to solve this problem is using [callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks). Using callbacks, we store our data at the end of each epoch. If Colab gets disconnected, we can load the latest weights and resume training from the last epoch. \n",
        "\n",
        "We have trained only for 15 epochs to accuracy 30%, which is low. We can train for more epochs to imporve this accuracy. However, there are more important parts for imporving the model, such as using a more complicated attention. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8gV07eLQSHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "60c97e01-7d31-4b95-d7f0-367b8a0b84cc",
        "id": "rQNPIpEsQTNH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from keras.callbacks import *\n",
        "filepath = \"/content/drive/My Drive/epochs:{epoch:03d}\"\n",
        "checkpoint = ModelCheckpoint(filepath, save_weights_only=True)\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "### Here we load the file of the already completed epochs, for example for epoch 10. \n",
        "# model.load_weights('/content/drive/My Drive/epochs:010')\n",
        "\n",
        "# model.fit([question_padded_, context_padded_], y_train_, epochs=num_epochs)\n",
        "\n",
        "num_epochs = 15\n",
        "in_ip = 0 # You must change this number if you are loading data from previous epcochs. \n",
        "\n",
        "history = model.fit(train_dataset, initial_epoch = in_ip , epochs=num_epochs, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/15\n",
            "292/292 [==============================] - 4620s 16s/step - loss: 5.5858 - costum_accuracy: 0.2861\n",
            "Epoch 12/15\n",
            "292/292 [==============================] - 4777s 16s/step - loss: 5.5282 - costum_accuracy: 0.2904\n",
            "Epoch 13/15\n",
            "292/292 [==============================] - 4543s 16s/step - loss: 5.4701 - costum_accuracy: 0.2978\n",
            "Epoch 14/15\n",
            "292/292 [==============================] - 4722s 16s/step - loss: 5.4160 - costum_accuracy: 0.3031\n",
            "Epoch 15/15\n",
            "292/292 [==============================] - 4445s 15s/step - loss: 5.3748 - costum_accuracy: 0.3054\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}